{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q5efTs1S-XAO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 28*28\n",
        "num_classes = 10\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "2YvV4qNQ-1xm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim = 1)\n",
        "    return(torch.tensor(torch.sum(preds == labels).item()/ len(preds)))"
      ],
      "metadata": {
        "id": "eT8Evl2z--q6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We put all of the above:\n",
        "class MnistModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "\n",
        "    def forward(self, xb):\n",
        "        xb = xb.reshape(-1, 784)\n",
        "        out = self.linear(xb)\n",
        "        return(out)\n",
        "\n",
        "    # We add extra methods\n",
        "    def training_step(self, batch):\n",
        "        # when training, we compute the cross entropy, which help us update weights\n",
        "        images, labels = batch\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        out = self(images) ## Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) ## Calculate the loss\n",
        "        return(loss)\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        out = self(images) ## Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) ## Calculate the loss\n",
        "        # in validation, we want to also look at the accuracy\n",
        "        # idealy, we would like to save the model when the accuracy is the highest.\n",
        "        acc = accuracy(out, labels) ## calculate metrics/accuracy\n",
        "        return({'val_loss':loss, 'val_acc': acc})\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        # at the end of epoch (after running through all the batches)\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()\n",
        "        return({'val_loss': epoch_loss.item(), 'val_acc' : epoch_acc.item()})\n",
        "\n",
        "    def epoch_end(self, epoch,result):\n",
        "        # log epoch, loss, metrics\n",
        "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
        "\n",
        "\n",
        "# a simple helper function to evaluate\n",
        "def evaluate(model, data_loader):\n",
        "    # for batch in data_loader, run validation_step\n",
        "    outputs = [model.validation_step(batch) for batch in data_loader]\n",
        "    return(model.validation_epoch_end(outputs))\n",
        "\n",
        "\n",
        "# actually training\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        ## Training Phase\n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            loss.backward() ## backpropagation starts at the loss and goes through all layers to model inputs\n",
        "            optimizer.step() ## the optimizer iterate over all parameters (tensors); use their stored grad to update their values\n",
        "            optimizer.zero_grad() ## reset gradients\n",
        "\n",
        "        ## Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return(history)"
      ],
      "metadata": {
        "id": "CFvmX1JW-v_e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=16,\n",
        "                kernel_size=5,\n",
        "                stride=1,\n",
        "                padding=2,\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, 5, 1, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        # fully connected layer, output 10 classes\n",
        "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        output = self.out(x)\n",
        "        return output, x    # return x for visualization\n",
        "\n",
        "def train(num_epochs, cnn, loaders):\n",
        "    cnn.train()\n",
        "    optimizer = optim.Adam(cnn.parameters(), lr = 0.01)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    # Train the model\n",
        "    total_step = len(loaders)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for i, (images, labels) in enumerate(loaders):\n",
        "\n",
        "            # gives batch data, normalize x when iterate train_loader\n",
        "            b_x = images.to(device)   # batch x\n",
        "            b_y = labels.to(device)   # batch y\n",
        "            output = cnn(b_x)[0]\n",
        "            loss = loss_func(output, b_y)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # clear gradients for this training step\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # backpropagation, compute gradients\n",
        "            loss.backward()\n",
        "            # apply gradients\n",
        "            optimizer.step()\n",
        "\n",
        "        print (f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss/total_step}')"
      ],
      "metadata": {
        "id": "DkrrLBL-_I5a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HOMEWORK 1**\n",
        "Build a classifier for fashion MNIST.\n",
        "\n",
        "**1. Use exactly the same architectures (both densely connected layers and from convolutional layers) as the above MNIST** e.g., replace the dataset. Save the Jupyter Notebook in its original format and output a PDF file after training, testing, and validation. Make sure to write down how do they perform (training accuracny, testing accuracy).  \n",
        "**2. Improve the architecture**. Experiment with different numbers of layers, size of layers, number of filters, size of filters. You are required to make those adjustment to get the highest accuracy. Watch out for overfitting -- we want the highest testing accuracy!\n",
        "Please provide a PDF file of the result, the best test accuracy and the architecture (different numbers of layers, size of layers, number of filters, size of filters)"
      ],
      "metadata": {
        "id": "jZe-igj__uyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # to tensor + scale to [0, 1]\n",
        "    transforms.Normalize((0.5,), (0.5,))  # normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the datasets\n",
        "train_dataset = datasets.FashionMNIST(\n",
        "    root='data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train\n",
        ")\n",
        "\n",
        "partitions = [int(len(train_dataset)*0.9), int(len(train_dataset)*0.1)]\n",
        "train_dataset, val_dataset = random_split(train_dataset, partitions)\n",
        "\n",
        "test_dataset = datasets.FashionMNIST(\n",
        "    root='data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_test\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "fashion_train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "fashion_validation_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "fashion_test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Frv0SXvT_f8O"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# linear net\n",
        "linear_model = MnistModel().to(device)\n",
        "\n",
        "train_metrics_linear = fit(5, 0.001, linear_model, fashion_train_loader, fashion_validation_loader)\n",
        "train_metrics_linear"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frfiJ7FS_jRg",
        "outputId": "2b40b29c-9c07-495b-f2aa-e160fa2f627d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0], val_loss: 1.1157, val_acc: 0.6642\n",
            "Epoch [1], val_loss: 0.9603, val_acc: 0.6943\n",
            "Epoch [2], val_loss: 0.8840, val_acc: 0.7138\n",
            "Epoch [3], val_loss: 0.8505, val_acc: 0.7180\n",
            "Epoch [4], val_loss: 0.8261, val_acc: 0.7256\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'val_loss': 1.1157022714614868, 'val_acc': 0.6642287373542786},\n",
              " {'val_loss': 0.9603449106216431, 'val_acc': 0.6943151354789734},\n",
              " {'val_loss': 0.8840186595916748, 'val_acc': 0.7138187289237976},\n",
              " {'val_loss': 0.8504688739776611, 'val_acc': 0.7180296778678894},\n",
              " {'val_loss': 0.8261046409606934, 'val_acc': 0.7256205677986145}]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics_linear = evaluate(linear_model, fashion_test_loader)\n",
        "test_metrics_linear"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Md5Wdr3_ldR",
        "outputId": "e5c1ec8c-cd57-406e-daee-8d372d6b196d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_loss': 0.7531313896179199, 'val_acc': 0.7481091022491455}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN net\n",
        "cnn_model = CNN().to(device)\n",
        "train(num_epochs=5, cnn=cnn_model, loaders=fashion_train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gHdZ4tT_nxP",
        "outputId": "dd8200c0-7ab4-4da7-b08c-f18965cc3d66"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.7304294432438381\n",
            "Epoch [2/5], Loss: 0.6249355729934164\n",
            "Epoch [3/5], Loss: 0.6130553808469343\n",
            "Epoch [4/5], Loss: 0.5978283848948953\n",
            "Epoch [5/5], Loss: 0.5913082581913867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in fashion_test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        test_output, last_layer = cnn_model(images)\n",
        "        pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
        "        acc = (pred_y == labels).sum().item() / float(labels.size(0))\n",
        "        pass\n",
        "print('Test Accuracy of the model on the 10000 test images: %.2f' % acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw7oXDpD_qrq",
        "outputId": "b6f5ee28-2c81-417f-d56c-78fcd7b1d9fe"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy of the model on the 10000 test images: 0.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=7, stride=1, padding=3),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.MaxPool2d(2, 2)  # Output: 7x7\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.linear_block = nn.Sequential(\n",
        "            nn.Linear(7 * 7 * 64, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear_block(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "27P4GKZr_3Se"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, device, lr=0.001, num_epochs=10):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            acc = accuracy(outputs, labels)\n",
        "\n",
        "            # Backward\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_acc += acc\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        avg_acc = running_acc / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_acc = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                acc = accuracy(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                val_acc += acc\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc /= len(val_loader)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "              f\"Train Loss: {avg_loss:.4f}, Acc: {avg_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "yNnboaJb_5IW"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_net = FashionClassifier()\n",
        "train_model(custom_net, fashion_train_loader, fashion_validation_loader, device, lr=0.001, num_epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXPmb-l5AZDH",
        "outputId": "6e9c2112-d642-42a8-f34c-86bee0892df2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5] Train Loss: 0.6327, Acc: 0.7716 | Val Loss: 0.3908, Acc: 0.8568\n",
            "Epoch [2/5] Train Loss: 0.4147, Acc: 0.8525 | Val Loss: 0.3368, Acc: 0.8737\n",
            "Epoch [3/5] Train Loss: 0.3644, Acc: 0.8699 | Val Loss: 0.3205, Acc: 0.8877\n",
            "Epoch [4/5] Train Loss: 0.3370, Acc: 0.8795 | Val Loss: 0.3001, Acc: 0.8892\n",
            "Epoch [5/5] Train Loss: 0.3170, Acc: 0.8872 | Val Loss: 0.2940, Acc: 0.8982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    test_acc = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            acc = accuracy(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            test_acc += acc\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_acc /= len(test_loader)\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "zRdRWpH9AcOn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(custom_net, fashion_test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUITxX7LAfC6",
        "outputId": "206e41ee-a7aa-47be-f078-f425e19a0227"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.2775, Accuracy: 0.9002\n"
          ]
        }
      ]
    }
  ]
}